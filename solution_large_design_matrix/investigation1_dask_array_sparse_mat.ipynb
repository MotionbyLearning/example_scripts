{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e1f2d23",
   "metadata": {},
   "source": [
    "# Handle large design matrix A in network unwrapping\n",
    "\n",
    "## The problem\n",
    "\n",
    "Consider we have a network with `N_points` points (nodes) and `N_arcs` arcs, each point has a wrapped phase time series with the length of `N_time`. By forming arcs, we have differential phases between the points, and we can solve the ambiguities per arc.\n",
    "\n",
    "Now we want to integrate the arcs ambiguities into point ambiguities. For a point, there can be multiple arcs connected to it, and if we approach a point following different arcs, the ambiguities solutions may conflict. Therefore we need to find an optimal solution to adjust these errors. A very simplified solution is through the following linear model:\n",
    "\n",
    "\n",
    "```math\n",
    "y = Ax\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "- `y` is the known arc ambiguity matrices, with the shape `N_arcs` x `N_time`\n",
    "- `A` is the design matrix, with the shape `N_arcs` x `N_pixels`. `A` represents the relationship between the arcs and the points, with values -1, 1, and 0. Each row of `A` represent to an arc, with 1 marking the target point of that arc, -1 for the source point.\n",
    "- `x` is the ambuiguity time series of the points need to be solved, with the shape `N_points` x `N_time`.\n",
    "\n",
    "Based on Least-Squares principle, assuming all `y` are independent and equally weighted, we can estimate `x` as:\n",
    "\n",
    "```math\n",
    "x = (A^T A)^{-1} A^T y\n",
    "```\n",
    "\n",
    "If we look into this very simple model, `A` is a very large sparse matrix. In this notebook, we can investigate if there is a simple way in python to perform this computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149e2a70",
   "metadata": {},
   "source": [
    "## Some conclusions\n",
    "In this notebook, there are three approaches to solve the above problem. Only the first approach finishes within a reasonable time. Based on the finding, I draw the following conclusion:\n",
    "\n",
    "- Dask sparse matrix (the `sparse` lib) is very efficient on handling large sparse matrix multiplication. However, it does not support matrix inversion.\n",
    "- `da.linalg.solve` works well for matrix inversion on Dask Array, however the array must be dense. It can handle large arrays with good memory management and performance.\n",
    "- It is possible to make the dense operation delayed on a Dask sparse Array, then put it in `da.linalg.solve`. However this seem to be over complicate the task graph and even defeat the chunking mechanism. It gives memory error.\n",
    "\n",
    "Based on the above findings, I would recommend Approach 1 for solving large matrix A. It solves a network with 10,000 points and 100,000 arcs in less than 5 minutes, on a local laptop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd6fc4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "import sparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd41ba3",
   "metadata": {},
   "source": [
    "## Setup experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af77a8",
   "metadata": {},
   "source": [
    "Even considering extremely large networks, we have:\n",
    "\n",
    "- N_points < 50000\n",
    "- N_arcs < 600000\n",
    "\n",
    "For a single point, its connection n_connection < 100\n",
    "\n",
    "In this test, we use a network with 10,000 points and 100,000 arcs. Each point has a time series of length 100. This will be larger than 90% cases in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cd65ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of points and arcs\n",
    "N_points = 10_000\n",
    "N_arcs = 100_000\n",
    "N_time = 100 # Only influence y_dummy\n",
    "\n",
    "# Dask setup\n",
    "N_chunks = 10\n",
    "\n",
    "# Dummy arc ambiguities\n",
    "y_dummy = da.random.randint(0, 1, (N_arcs, N_time), chunks=(N_arcs // N_chunks, N_time))\n",
    "\n",
    "# randomly generate connections between points\n",
    "start_end_indices = da.random.randint(0, N_points, size=(N_arcs, 2), chunks=(N_arcs//10, 2)).astype(np.int32).compute()\n",
    "xindex = np.arange(N_arcs)\n",
    "yindex_start = start_end_indices[:, 0]\n",
    "yindex_end = start_end_indices[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620e7af4",
   "metadata": {},
   "source": [
    "## Solution 1: Build A as a dask sparse matrix\n",
    "\n",
    "This approach create A as a dask sparse matrix, which takes very little memory. The disadavantage is that we need to persist `A^T A` in memory, since the matrix inversion process `da.linalg.solve` does not support sparse matrix. This assumes that `A^T A`, which has the size of `N_point` x `N_point` can fit into memory. Considering `A`has a dtype of `np.int8`. For `A^T A`, since the conncetion per node is usually < 100, therefore it can also be represented as `np.int8` (range -128 to 127). Therefore, `N` is likely to be able to fit into memory.\n",
    "\n",
    "For N_points = 50000, the memory usage of `A^T A` is:\n",
    "\n",
    "```python\n",
    "N_points = 50000\n",
    "N_bytes = N_points * N_points * np.dtype(np.int8).itemsize\n",
    "N_bytes / 1024**2  # in MB\n",
    "```\n",
    "\n",
    "Which is about 2.4GB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5f39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sparse COO matrix\n",
    "A_sparse_start = sparse.COO((xindex, yindex_start), np.full_like(xindex, -1, dtype=np.int8), shape=(N_arcs, N_points))\n",
    "A_sparse_end = sparse.COO((xindex, yindex_end), np.full_like(xindex, 1, dtype=np.int8), shape=(N_arcs, N_points))\n",
    "A_sparse = A_sparse_start + A_sparse_end\n",
    "# Convert to Dask array with desired chunks\n",
    "A = da.from_array(A_sparse, chunks=(N_arcs//N_chunks, N_points//N_chunks))\n",
    "\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80e3d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One can perform matrix multiplication on sparse array A,\n",
    "# But inverse is not supported for sparse arrays\n",
    "# We can compute N (N_point x N_point) as a dense array, since it is smaller than A (N_points < N_arcs)\n",
    "N = (A.T @ A).astype(np.int8)\n",
    " \n",
    "# Dask array N does not have a .todense() method\n",
    "# delayed dense N for linear solve, but still make the computation fit in memory\n",
    "N_dense = N.map_blocks(lambda x: x.todense(), dtype=N.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d06331",
   "metadata": {},
   "source": [
    "The following code is commented out because it give memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f1da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If we directly use N_dense as a Dask array in da.linalg.solve, we will run into memory errors\n",
    "# param_dummy = (\n",
    "#     da.linalg.solve(\n",
    "#         N_dense, da.eye(N_points, chunks=N_points // N_chunks, dtype=N.dtype)\n",
    "#     )\n",
    "#     @ A.T\n",
    "#     @ y_dummy\n",
    "# )\n",
    "\n",
    "# param_dummy_comp = param_dummy.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07174452",
   "metadata": {},
   "source": [
    "The following code works, if we first persist `N = A.T @ A` in memory, and then use `da.linalg.solve` to solve the linear system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d319c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The whole cell take about 3 minites to run\n",
    "# Solving N_dense_inv seem to be a must\n",
    "# da.linalg.solve(N_dense, A_dense) will cause memory issues since A_dense can not fit in memory\n",
    "# da.linalg.solve only works with dense matrices\n",
    "N_dense_inv = da.linalg.solve(N_dense, da.eye(N_points, chunks=N_points//N_chunks, dtype=N.dtype))\n",
    "N_dense_inv_comp = N_dense_inv.compute() # About 2mins 39s to compute\n",
    "\n",
    "# Create a Dask Array from the computed inverse matrix in memory\n",
    "da_N_dense_inv_comp = da.from_array(N_dense_inv_comp, chunks=N_points//N_chunks)\n",
    "\n",
    "# Linear solve to find parameters\n",
    "# param_dummy = N_dense_inv @ A.T @ y_dummy\n",
    "param_dummy = da_N_dense_inv_comp @ A.T @ y_dummy\n",
    "param_dummy_comp = param_dummy.compute() # This takes ~30s to compute\n",
    "param_dummy_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d875de9",
   "metadata": {},
   "source": [
    "## Solution 2 (Not Working): First dense A\n",
    "This assumes `A` can fit into memory. If this is the case then we dense `A` from the beginning, and create dask array from it. This is an attemp to accelerate the computation but it is not successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b14d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sparse COO matrix\n",
    "A_sparse_start = sparse.COO((xindex, yindex_start), np.full_like(xindex, -1, dtype=np.int8), shape=(N_arcs, N_points))\n",
    "A_sparse_end = sparse.COO((xindex, yindex_end), np.full_like(xindex, 1, dtype=np.int8), shape=(N_arcs, N_points))\n",
    "A_sparse = A_sparse_start + A_sparse_end\n",
    "# Convert to Dask array with desired chunks\n",
    "A = da.from_array(A_sparse.todense(), chunks=(N_arcs//N_chunks, N_points//N_chunks))\n",
    "\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c610bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This does not finish in 30 minutes\n",
    "N = (A.T @ A).astype(np.int8)\n",
    "param_dummy = (\n",
    "    da.linalg.solve(\n",
    "        N, da.eye(N_points, chunks=N_points // N_chunks, dtype=N.dtype)\n",
    "    )\n",
    "    @ A.T\n",
    "    @ y_dummy\n",
    ")\n",
    "param_dummy_comp = param_dummy.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d4b7a0",
   "metadata": {},
   "source": [
    "## Solution 3 (not working): Create A directly as a densed Dask array, without sparsity\n",
    "\n",
    "In this method, it is attempted to create `A` as a dask array without sparsity, from the coordinates. Despite the fact that there is no memory error, the computation is extremely slow. Computing N does not finish in 10 minutes.\n",
    "\n",
    "Maybe there is a better way to create `A` without block-wisely looping coordinates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e499dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dense Dask array of zeros\n",
    "A = da.zeros((N_arcs, N_points), dtype=np.int8, chunks=(N_arcs//N_chunks, -1))\n",
    "\n",
    "# Create a function to set the 1 and -1 in the Dask array\n",
    "# TODO:Is there a better way to do this? It seems Dask Array does not support setting values by index\n",
    "def set_numbers(block, xindex, yindex_start, yindex_end, block_info=None):\n",
    "    # in block, set (xindex, yindex_start) to -1 and (xindex, yindex_end) to 1\n",
    "    if block_info is None:\n",
    "        return block\n",
    "\n",
    "    # Get the block location in global space\n",
    "    arr_loc = block_info[None]['array-location']\n",
    "    row_start, row_end = arr_loc[0]\n",
    "    col_start, col_end = arr_loc[1]\n",
    "    xindex_block = xindex[row_start:row_end]\n",
    "    yindex_start_block = yindex_start[row_start:row_end]\n",
    "    yindex_end_block = yindex_end[row_start:row_end]\n",
    "\n",
    "    # Set the values in the block\n",
    "    result = block.copy()\n",
    "    result[xindex_block - row_start, yindex_start_block] = -1\n",
    "    result[xindex_block - row_start, yindex_end_block] = 1\n",
    "\n",
    "    return result\n",
    "\n",
    "# Apply\n",
    "A = A.map_blocks(set_numbers, xindex, yindex_start, yindex_end, dtype=A.dtype).rechunk((N_arcs // N_chunks, N_points // N_chunks))\n",
    "\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e0f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = (A.T @ A).astype(np.int8)\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e22b6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This does not finish in 30 mins\n",
    "N_comp = N.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_inv = da.linalg.solve(N, da.eye(N_points, chunks=N_points//N_chunks, dtype=N.dtype))\n",
    "param_dummy = N_inv @ A.T @ y_dummy\n",
    "param_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4437979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes\n",
    "param_dummy_comp = param_dummy.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depsi-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
